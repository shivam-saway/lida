version: '3.8'
services:
  llama-cpp-python-server:
    image: ghcr.io/abetlen/llama-cpp-python:v0.3.5
    container_name: llama_cpp_python_container
    ports:
      - "8000:8000"
    volumes:
      - C:/Users/<UserName>/.cache/huggingface/hub/<gguf-type-model>/snapshots/<hash>:/models/<model-name>
    environment:
      - MODEL=/models/<model-name>/<model-file>.gguf # We can use diffrent quantized version of model like Phi-3-mini-4k-instruct-q4.gguf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  lida:
    image: lida:latest
    build:
      context: .
      dockerfile: dockerfile
    container_name: lida_container
    ports:
      - "8080:8080"
    environment:
      # Point LIDA to the local LLM server using OpenAI-compatible API
      - LIDA_LLM_PROVIDER=openai
      - OPENAI_BASE_URL=http://llama-cpp-python-server:8000/v1
      - LIDA_LLM_API_BASE=http://llama-cpp-python-server:8000/v1
      - OPENAI_API_KEY=EMPTY
      - LIDA_LLM_MODEL=phi-3
      - LIDA_LLM_MODELS=[{"name":"phi-3","max_tokens":2048,"model":{"provider":"openai","parameters":{"model":"phi-3"}}}]
    depends_on:
      - llama-cpp-python-server
